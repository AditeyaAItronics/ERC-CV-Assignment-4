{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Write a Python program using MediaPipe to detect and draw hand contours on a live webcam feed. The program should:\n",
    "\n",
    "Use a live feed from the webcam to continuously detect hands. Draw the detected hand landmarks and connections (contours) on the video frame in real-time. Flip the frame horizontally for a mirror effect. Stop the video feed when the user presses the 'q' key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working behind Hand Detection:\n",
    "\n",
    "\n",
    "## MediaPipe\n",
    "\n",
    "\n",
    "MediaPipe is an open-source Python library developed by Google that provides a comprehensive suite of tools for building applications that process and analyze multimedia data, such as images and videos. It offers a wide range of pre-built machine learning models and pipelines for tasks like facial recognition, hand tracking, pose estimation, object detection, and more. MediaPipe simplifies the development of computer vision, making it accessible to developers with various levels of expertise. It is often used in applications related to augmented reality, gesture recognition, and real-time tracking, among others.\n",
    "\n",
    "## Hand Landmarks in MediaPipe\n",
    "\n",
    "In MediaPipe, hand landmarks refer to the precise points or landmarks detected on a human hand in an image or video. The library's Hand module provides a machine learning model that can estimate the 21 key landmarks on a hand, including the tips of each finger, the base of the palm, and various points on the fingers and hand. These landmarks can be used for various applications, such as hand tracking, gesture recognition, sign language interpretation, and virtual reality interactions. MediaPipe's hand landmarks model makes it easier for developers to create applications that can understand and respond to hand movements and gestures in real-time.\n",
    "\n",
    "## Reference material\n",
    "https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker https://pypi.org/project/mediapipe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands and Drawing utils so that we can use them later\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the webcam of the laptop as this will be fed directly\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MediaPipe Hands model with default settings\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame and nothing to do.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally for a mirror effect as adviced\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame from BGR to RGB due to reading from open CV\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the RGB frame to detect hands as that is our objective in the assignment\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Draw hand landmarks and connections if any hand is detected as requested in the problem assignment\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the processed video frame\n",
    "        cv2.imshow('Hand Contours', frame)\n",
    "\n",
    "        # Press 'q' to quit from the frame reading\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
